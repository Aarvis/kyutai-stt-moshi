# **Moshi-Finetune** 

## üöÄ Introduction

<div style="text-align: center;">
  <img src="./moshi_finetune_logo.png" alt="Moshi interface" width="250px" style="margin-left: 20px;">
</div>

**Moshi-Finetune** provides an easy way to fine-tune [Moshi models](https://github.com/kyutai-labs/moshi)
using **LoRA (Low-Rank Adaptation)** for lightweight and efficient training. This guide walks you through
installation, model downloading, dataset preparation, training, and inference. By following these steps,
you'll be able to: transform stereo audio files into your very own transcribed dataset, fine-tune
[moshi weights](https://huggingface.co/kyutai/moshiko-pytorch-bf16) on real conversations, and‚Äîbest of
all‚Äîchat with your customized moshi model!

## üì• Installation

To get started, follow these steps:

### 1Ô∏è‚É£ Clone this repository
```sh
cd $HOME && git clone git@github.com:kyutai-labs/moshi-finetune.git
```

### 2Ô∏è‚É£ Install all required dependencies:
You will need at least Python 3.10, with 3.12 recommended. Then, run:

```sh
cd moshi-finetune
pip install -r requirements.txt
```

Note that this installs the `lora` branch of [moshi](https://github.com/kyutai-labs/moshi)
as the necessary changes have not been merged into the main branch yet.

## üì• Model Configuration

The training setup is specified via a YAML configuration file. The example
configuration files are located in the `example` directory.

We recommend fine-tuning one of the official moshi models. To achieve this, you
can use the following section in your configuration file.

```
moshi_paths:
   hf_repo_id: str  = "kyutai/moshiko-pytorch-bf16"
   mimi_path: str = "tokenizer-e351c8d8-checkpoint125.safetensors"
   moshi_path: str  = "model.safetensors"
   tokenizer_path: str  = "tokenizer_spm_32k_3.model"
```

## üìö Prepare Dataset

The pipeline expects a dataset of stereo audio files, the left channel is used
for the audio generated by moshi, whereas the second channel is used for the
user's input.

The collection of audio files should be detailed in a `.jsonl` file.
Each line having a `path` and a `duration` field. The duration is given in
seconds. Each audio file should have an associated `.json` file that contains
the transcript with timestamps.

A sample dataset in this format can be found in the
[kyutai/DailyTalkContiguous](https://huggingface.co/datasets/kyutai/DailyTalkContiguous)
repository. This dataset can be retrieved using the following snippet:
```python
from huggingface_hub import snapshot_download
local_dir = snapshot_download(
    "kyutai/DailyTalkContiguous",
    repo_type="dataset",
    local_dir="./daily-talk-contiguous")
```

If you want to annotate your own dataset and generate the `.json` files for each
audio file, you can use the `annotate.py` script:

```sh
python annotate.py {your jsonl file} -S=64 --partition='your-partition'
```

## üèãÔ∏è Start Training

Once your dataset is ready, start fine-tuning using the following steps.

#### üìå Recommended settings for quick training:
```
lora:
  enable: true
  rank: 128
  scaling: 2.


duration_sec: 100
batch_size: 16
max_steps: 2000
```

Optionally you can also set `wandb`

#### üìå Run training on multiple GPUs (8):
```sh
torchrun --nproc-per-node 8 --master_port $RANDOM -m train example/moshi_7B.yaml
```

#### üí° Expected Training Times:
Using the above hyperparameters:

|  | Avg Tokens/sec   | Peak Allocated Memory   |
|------|------|------|
|   1√óH100  | ‚âà17k| 39.6GB |
|   8√óH100  | ‚âà15.2k| 23.7GB |



If you encounter **Out-Of-Memory errors**, try reducing the `batch_size`. If the issue persists, you can lower the `duration_sec` parameter, but be aware that this may negatively impact the user experience during inference, potentially causing the model to become silent more quickly.

## ‚öôÔ∏è Customizing Training Configuration

The example `moshi-finetune/example/7B` defines reasonable parameters for learning rate, weight decay, etc... but you are advised to
customize these settings for your use case.


### **üîß Key Training Parameters**
| Parameter              | Description |
|------------------------|-------------|
| `moshi_paths`         | Defines all the paths: `.hf_repo_id` if the model is imported from Hugging Face Hub (+ models' names instead of path), or directly `.mimi_path` and `.moshi_path` for local weights. `.tokenizer_path` must also be specified. |
| `run_dir`             | Directory where training checkpoints and logs are stored. |
| `duration_sec`        | Maximum sequence length (in seconds) for training. |
| `first_codebook_weight_multiplier` | The first codebook being the semantic token, we put more weight on it. | 
| `text_padding_weight` | Most of the text stream is padding as audio is 12.5Hz with mimi but tokenizing text takes less space. Decrease the loss weight on paddings to avoid the model over-focussing on predicting paddings. | 
| `gradient_checkpointing` | Whether to use gradient checkpointing per transformer layer to mitigate out of memory issues. |
| `batch_size`         | Number of training examples per GPU. |
| `max_steps`         | Total number of training steps. Defines how many iterations the training will run. **Total tokens processed = max_steps √ó num_gpus √ó batch_size √ó duration_seq √ó 9 (token per step) √ó 12.5 (step per second) **. |
| `optim.lr`          | Learning rate. Recommended starting value: **2e-6**. |
| `optim.weight_decay` | Weight decay for regularization. Default: **0.1**. |
| `optim.pct_start`   | Percentage of total training steps used for learning rate warm-up before decay. Equivalent to `pct_start` in PyTorch‚Äôs `OneCycleLR`. |
| `lora.rank`         | Size of the **LoRA adapters**. Recommended **‚â§128** for efficiency. |
| `seed`              | Random seed for initialization, data shuffling, and sampling (ensures reproducibility). |
| `log_freq`          | Defines how often (in steps) training metrics are logged. |
| `data.train_data`   | Path to the dataset used for training. |
| `data.eval_data`    | (Optional) Path to evaluation dataset for cross-validation at `eval_freq` intervals. |
| `data.shuffle`      | Whether to shuffle training samples (Recommended). |
| `eval_freq`        | Number of steps between evaluations on the validation set. |
| `no_eval`         | If `False`, enables periodic model evaluation during training. |
| `ckpt_freq`       | Number of steps between saving model checkpoints. |
| `full_finetuning` | Set to `True` for **full fine-tuning**, or `False` to use **LoRA** for adaptation. |
| `save_adapters`  | If `True`, saves only **LoRA adapters** (works with [Moshi Inference](https://github.com/kyutai-labs/moshi)). If `False`, merges LoRA into the base model (requires sufficient CPU/GPU memory). |
| `wandb.key`      | API key for **Weights & Biases (wandb)** logging (Optional). |
| `wandb.project`  | Name of the **wandb project** where training logs will be stored. |


## üîÆ Inference

#### 1Ô∏è‚É£ Install Moshi for inference

Once your model is trained, you can use it in interactive mode using [moshi](https://github.com/kyutai-labs/moshi).
The package should already be in your environment if you used the
`requirements.txt` file. If not, you can install it using the following command:
```sh
pip install -U -e "git+https://github.com/kyutai-labs/moshi.git@lora#egg=moshi&subdirectory=moshi"
```

#### 2Ô∏è‚É£ Run inference using the fine-tuned model
Assuming your `lora.safetensors` is saved under `$HOME/dailydialog_ft/checkpoints/checkpoint_000500/consolidated/lora.safetensors`, and that you want to use the same base model as the one fine-tuned, you can discuss with the model using `moshi`, *e.g.*:

```sh
python -m moshi.server --lora-folder=$HOME/dailydialog_ft/checkpoints/checkpoint_000500/consolidated/
```

Assuming your `lora.safetensors` is saved under `$HOME/dailydialog_ft/checkpoints/checkpoint_000300/consolidated/lora.safetensors`, and that you want another base model available (in Hugging Face Hub `kyutai/moshiko-pytorch-bf16` for instance), you can discuss with the model using `moshi`, *e.g.*:

```sh
python -m moshi.server --lora-weight=$HOME/dailydialog_ft/checkpoints/checkpoint_000500/consolidated/lora.safetensors --hf-repo=kyutai/moshiko-pytorch-bf16
```


## üìä Monitoring with Weights & Biases (W&B)

Explicit support for [Weights and Biases](https://www.wandb.com/) are added to help you monitor and visualize your training runs. This integration allows you to log various metrics and track experiments easily.

### Setting Up Weights and Biases

To use Weights and Biases with `moshi-finetune`, follow these steps:

1. **Install Weights and Biases:**

   Make sure you have the `wandb` library installed. You can install it using pip:

```sh
   pip install wandb
```
### Viewing Your Logs

Once the training starts, you can monitor the progress in real-time by visiting your wandb project dashboard. All metrics, including training loss, evaluation loss, learning rate, etc., will be logged and visualized.

For more details on how to use wandb, visit the [Weights and Biases documentation](https://docs.wandb.ai/).

## Acknowledgments

This project uses code from [mistral-finetune](https://github.com/mistralai/mistral-finetune) licensed under the Apache License 2.0.
