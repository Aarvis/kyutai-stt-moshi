# Example configuration for fine-tuning a LoRA adapter for voice recognition (train_stt.py)
# Note: as of 2025-07-16 requires a hacked version of lm.py. Dataset format as in kyutai/DailyTalkContiguous, but mono.
# See https://github.com/kyutai-labs/delayed-streams-modeling/issues/4#issuecomment-3079816523 
batch_size: 10
ckpt_freq: 10
data:
  eval_data: 'eval_data/test_data.jsonl'
  shuffle: true
  train_data: '10_data/train_data.jsonl' # Fill
do_eval: true
duration_sec: 10
eval_freq: 2
first_codebook_weight_multiplier: 100.0
full_finetuning: true
gradient_checkpointing: true
log_freq: 10
lora:
  enable: false
  ft_embed: false
  rank: 32
  scaling: 2.0
max_steps: 100
moshi_paths:
  hf_repo_id: kyutai/stt-1b-en_fr
  moshi_path: null                          # let it pick the default from HF
  mimi_path: null                           # idem
  tokenizer_path: "../hindi_tokenizer/hi_sp.model"    # ← your SPM file on disk
  config_path: "../hindi_tokenizer/config.json" # ← the edited config from (B)
optim:
  lr: 2.0e-05
  pct_start: 0.05
  weight_decay: 0.1
run_dir: 'run_dir' # Fill
save_adapters: false
seed: 0
text_padding_weight: 0.5
